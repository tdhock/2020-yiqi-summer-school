\documentclass[12pt]{article}
\usepackage{setspace}
\doublespacing
\usepackage{times}
\usepackage{natbib}
\usepackage{fullpage}
\usepackage{tikz}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{algorithmic}
\usepackage{multirow}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\Lik}{Lik}
\DeclareMathOperator*{\PoissonLoss}{PoissonLoss}
\DeclareMathOperator*{\Peaks}{Peaks}
\DeclareMathOperator*{\Segments}{Segments}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\sign}{\operatorname{sign}}
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\RR}{\mathbb R}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\NN}{\mathbb N}

\begin{document}

\title{Introduction to machine learning and neural networks}

\author{
  Toby Dylan Hocking\\
  toby.hocking@nau.edu
}

\maketitle


% Your manuscript is 20 or fewer pages, double space, including figures and tables, for each lecture or practice. That makes about 10 book pages. Exception is possible for a few lectures.

% Each manuscript may include 3-5 illustrations.

% Manuscript should be written at a level to be understood by first-year graduate students. 

% Each manuscript has a short, first paragraph (<50 words) about the training goal.

% Do not have too many references as it is not a research paper but list a few suggested readings.

% Include a few (4-6) quizzes for each lecture and expected homework for each practice.

% Text in your chapter should be corresponding to the pre-recorded video or the video to be recorded.

\paragraph{Objective.} This chapter introduces basic concepts and
algorithms from machine learning. The goal is to learn how to use
cross-validation for training and testing machine learning algorithms,
and to implement neural networks in R code.

\section{Introduction and applications of machine learning}

Machine learning is the domain of computer science which is concerned
with efficient algorithms for making predictions in all kinds of big
data sets. A defining characteristic of supervised machine learning
algorithms is that they require a data set for training. The machine
learning algorithm then memorizes the patterns present in those
training data, with the goal of accurately predicting similar patterns
in new test data. Many machine learning algorithms are
domain-agnostic, which means they have been shown to provide highly
accurate predictions in a wide variety of application domains
(computer vision, speech recognition, automatic translation, biology,
medicine, climate science, chemistry, geology, etc). 

\begin{figure} 
  \centering
  \includegraphics[width=0.8\textwidth]{drawing-mnist-train-test.pdf}
  \caption{A learning algorithm inputs a train data set, and outputs a
    prediction function, g or h. Both g and h input a grayscale image
    and output a class (integer from 0 to 9), but g is for  
    digits and h is for fashion.}
  \label{fig:drawing-mnist-train-test}
\end{figure}

For example, consider the problem of image classification from the
application domain of computer vision. In this problem, we would like
a function that can input an image, and output an integer which
indicates class membership. More precisely, let us consider the MNIST
and Fashion-MNIST data sets
(Figure~\ref{fig:drawing-mnist-train-test}), in which each input is
grayscale image with height and width of 28 pixels
\citep{LeCun1998,Xiao2017} then we typically represent each input as a
matrix of real numbers $\mathbf x\in\mathbb R^{28\times 28}$. In both
the MNIST and Fashion-MNIST data sets each image has a corresponding
label which is an integer $y\in\{0,1,\dots,9\}$. In the MNIST data set
each image/label represents a digit, whereas in Fashion-MNIST each
image/label represents a category of clothing (0 for T-shirt/top, 1
for Trouser, 2 for Pullover, etc).  In both data sets the goal is to
learn a function
$f:\mathbb R^{28\times 28}\rightarrow \{0,1,\dots, 9\}$ which inputs
an image $\mathbf x$ and outputs a predicted class $f(\mathbf x)$
which should ideally be the same as the corresponding label $y$.

As mentioned above, a big advantage of supervised learning algorithms
is that they are typically domain-agnostic, meaning that they can
learn accurate prediction functions $f$ using data sets with different
kinds of patterns. That means we can use a single learning algorithm
\textsc{Learn} on both the MNIST or Fashion-MNIST data sets
(Figure~\ref{fig:drawing-mnist-train-test}, left). For the MNIST data
set the learning algorithm will output a function for predicting the
class of digit images, and for Fashion-MNIST the learning algorithm
will output a function for predicting the class of a clothing image
(Figure~\ref{fig:drawing-mnist-train-test}, right). The advantage of
this supervised machine learning approach to image classification is
that the programmer does not need any domain-specific knowledge about
the expected pattern (e.g., shape of each digit, appearance of each
clothing type). Instead, we assume there is a data set with enough
labels for the learning algorithm to accurately infer the
domain-specific pattern and prediction function. This means that the
machine learning approach is only appropriate when it is
possible/inexpensive to create a large labeled data set that
accurately represents the pattern/function to be learned.

How do we know if the learning algorithm is working properly? The goal
of supervised learning is \keyword{generalization}, which means the
learned prediction function $f$ should accurately predict
$f(\mathbf x) = y$ for any inputs/outputs $(\mathbf x,y)$ that will be
seen in a desired application (including new data that were not seen
during learning). To formalize this idea, and to compute quantitative
evaluation metrics (accuracy/error rates), we need a test data set, as
explained in the next section.

\subsection{$K$-fold cross-validation for evaluating prediction/test accuracy}

Each input $\mathbf x$ in a data set is typically represented as one
of $N$ rows in a design matrix with $D$ columns (one for each
dimension or feature). Each output $y$ is represented as an element of
a label vector of size $N$, which can be visualized as another column
alongside the design matrix (Figure~\ref{fig:cross-validation},
left). For example, in the image data sets discussed above we have
$N=60,000$ labeled images/rows, each with $D=784$ dimensions/features
(one for each of the $28\times 28$ pixels in the image).

The goal of supervised learning is to find a prediction function $f$
such that $f(\mathbf x) = y$ for all inputs/outputs $(\mathbf x,y)$ in
a test data set (which is not available for learning
$f$). So how do we learn $f$ for accurate prediction on a test data
set, if that test set is not available? We must assume that we have
access to a train data set with the same statistical distribution as
the test data. The train data set is used to learn $f$, and the test
data can only be used for evaluating the prediction accuracy/error of
$f$.

Some benchmark data sets which are used for machine learning research,
like MNIST and Fashion-MNIST, have designated train/test
sets. However, in most applications of machine learning to real data
sets, train/test sets must be created. One approach is to create a
single train/test split by randomly assigning a set to each of the $N$
rows/observations, say 50\% train rows and 50\% test rows. The
advantage of that approach is simplicity, but the drawback is that we
can only report accuracy/error metrics with respect to one test set
(e.g., the algorithm learned a function which accurately predicted
91.3\% of observations/labels in the test set, meaning 8.7\% error
rate). 

In addition to estimating the accuracy/error rate, is important to
have some estimate of variance in order to make statements about
whether the prediction accuracy/error of the learned function $f$ is
significantly larger/smaller than other prediction functions. The
other functions to compare against may be from other supervised
learning algorithms, or some other method that does not use machine
learning (e.g., a domain-specific physical/mechanistic model). A common
baseline is the constant function $f(\mathbf x) = y_0$ where $y_0$ is
the average or most frequent label in the train data. This baseline
ignores all of the inputs/features $\mathbf x$, and can be used to
show that the algorithm is learning some non-trivial predictive relationship
between inputs and outputs (for an example see
Figure~\ref{fig:test-accuracy}).

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{drawing-cross-validation}
  \caption{$K=3$ fold cross-validation. \textbf{Left:} the first step
    is to randomly assign a fold ID from 1 to $K$ to each of the
    observations/rows. \textbf{Right:} in each of the
    $k\in\{1, \dots, K\}$ splits, the observations with fold ID $k$
    are set aside as a test set, and the other observations are used
    as a train set to learn a prediction function (f1--f3), which is
    used to predict for the test set, and to compute accuracy metrics
    (A1--A3).}
  \label{fig:cross-validation}
\end{figure}

The $K$-fold cross-validation procudure generates $K$ splits, and can
therefore be used to estimate both mean and variance of prediction
accuracy/error. The number of folds/splits $K$ is a user-defined
integer parameter which must be at least 2, and at most $N$. Typical
choices range from $K=3$ to 10, and usually the value of $K$ does not
have a large effect on the final estimated mean/variance of prediction
accuracy/error. The algorithm begins by randomly assigning a fold ID
number (integer from 1 to $K$) to each observation
(Figure~\ref{fig:cross-validation}, left). Then for each unique fold
value from 1 to $K$, we hold out the corresponding observations/rows
as a test set, and use data from all other folds as a train set
(Figure~\ref{fig:cross-validation}, right). Each train set is used to
learn a corresponding prediction function, which is then used to
predict on the held out test data. Finally, accuracy/error metrics are
computed in order to quantify how well the predictions fit the labels
for the test data. Overall for each data set and learning algorithm
the $K$-fold cross-validation procedure results in $K$ splits, $K$
learned functions, and $K$ test accuracy/error metrics, which are
typically combined by taking the mean and standard deviation (or
median and quartiles). Other algorithms may be used with the same fold
assignments, in order to compare algorithms in terms of accuracy/error
rates in particular data sets.

For example, Figure~\ref{fig:test-accuracy} uses $K=4$-fold
cross-validation to compare four learned functions on an image
classification problem. The accuracy rates of the ``dense'' and
``linear'' functions, $97.4 \pm 1.6 \%$ and $96.3 \pm 1.9 \%$ (mean
$\pm$ standard deviation) are not significantly different. Both rates
are significantly larger than the accuracy of the ``baseline''
constant function, $16.4 \pm 1.4 \%$, and smaller than the accuracy of
the ``conv'' function, $99.3 \pm 1.1 \%$. We can therefore conclude
that the most accurate learning algorithm for this problem, among
these four candidates, is the ``conv'' method (which uses a
convolutional neural network, explained later).  It is important to
note that statements about what algorithm is most accurate only makes
sense in terms of a particular data set, after having performed
$K$-fold cross-validation to estimate prediction accuracy/error rates.

\subsection{$K$-fold cross-validation for avoiding overfitting}

% \begin{figure}
%   \centering
%   \includegraphics[width=0.4\textwidth]{figure-fashion-mnist-one-example}
%   \includegraphics[width=0.4\textwidth]{screenshot-8-matrix-values}
%   \caption{One example/observation with label 8 in the MNIST data
%   set. \textbf{Left:} representation as greyscale
%   image. \textbf{Right:} representation as $28 \times 28$ matrix;
%   each of the 784 elements is an integer value from 0 to 255.}
%   \label{fig:mnist-one-example}
% \end{figure}

% \begin{figure}
%   \centering
%   \includegraphics[width=0.49\textwidth]{figure-fashion-mnist-digits}
%   \includegraphics[width=0.49\textwidth]{figure-fashion-mnist-fashion}
%   %   \\
%   %   \includegraphics[width=0.49\textwidth]{figure-fashion-mnist-digits-design}
%   %   \includegraphics[width=0.49\textwidth]{figure-fashion-mnist-fashion-design} 
%   \caption{Seven example images from each class/label in the original MNIST digits
%   (left) and Fashion-MNIST (right) data sets. For each data set
%   different label values from 0 to 9 appear are shown in panels
%   from left to right, and different examples/observations are
%   shown in panels from top to bottom.} 
%   \label{fig:fashion-mnist}
% \end{figure}

\subsection{Other applications}

Machine learning for cell image classification (CellProfiler).
Jones {\it et al.} PNAS 2009. Scoring diverse cellular morphologies in
image-based screens with iterative feedback and machine learning.
\begin{itemize}
  \item Input $x$ = color image of cell, 
  \item Output $y\in\{\text{yes}, \text{no}\}$ (binary classification),
\end{itemize}

{Machine learning for image segmentation}
Russell {\it et al.} 2007. LabelMe paper.
Q: What are the types/dimensions of $x,y,f$ in this example?

{Machine learning for spam filtering (Gmail)}
Want: $f$(email message)$\in\{0,1\}$ -- binary classification, spam=1
or not=0.

{Machine learning for translation (google books)}

{Machine learning for translation (google translate)}
Want: f(english)=french

{Machine learning for medical diagnosis}
want: f(image) = blood pressure, heart attack or stroke risk

\section{Demonstration of overfitting in regression /
  first steps with R}

{Machine learning algorithms in practice}
To learn a good prediction function for any of these problems (and
any other problems you encounter in your work) you should
\begin{itemize}
\item First gather a data set that you will use to train/test the
  machine learning algorithms, typically a CSV file with rows for
  observations and columns for features.
\item Use K-fold Cross-Validation to repeatedly divide the
  observations into train/test sets. Use the train set to learn
  machine learning model parameters, and use the test set to
  estimate prediction error.
\item Try a variety of different learning algorithms (e.g. linear
  models, random forests, boosting, neural networks, support vector
  machines, ...), because you don't know which one will result in
  most accurate predictions for any given data set. 
\item Each learning algorithm has different hyper-parameters which
  control complexity/regularization/overfitting, and typically must
  be chosen by the user (you) by dividing the train data into
  subtrain/validation sets. Use the hyper-parameter which is
  best with respect to the held-out validation set.
\end{itemize}

{Goal of this section: demonstrate overfitting}
\begin{itemize}
\item The goal of supervised machine learning is to get accurate
  predictions on new/unseen/held-out test data.
\item Any machine learning algorithm is prone to overfit, which
  means providing better predictions on the train/subtrain set than
  on a held-out validation/test set. (BAD)
\item To learn a model which does NOT overfit (GOOD), you need to
  first divide your train set into subtrain/validation sets.
\item Code for figures in this section:
  \url{https://github.com/tdhock/2020-yiqi-summer-school/blob/master/figure-overfitting.R}
\end{itemize}

\begin{figure}
  \centering
  \includegraphics[width=0.55\textwidth]{figure-overfitting-paper}
  \includegraphics[width=0.4\textwidth]{figure-overfitting-paper-loss}   
  \caption{Illustration of underfitting and overfitting in a neural
    network regression model (single hidden layer, 50 hidden
    units). \textbf{Left:} noisy data with a nonlinear sine wave
    pattern (grey circles), learned functions (colored curves), and
    residuals/errors (black line segments) are shown for three values
    of epochs (panels from left to right) and two data subsets (panels
    from top to bottom). \textbf{Right:} in each epoch the model
    parameters are updated using gradient descent with respect to the
    subtrain loss, which decreases with more epochs. The
    optimal/minimum loss with respect to the validation set occurs at
    64 epochs, indicating underfitting for smaller epochs (green
    function, too regular/linear for both subtrain/validation sets)
    and overfitting for larger epochs (purple function, very
    irregular/nonlinear so good fit for subtrain but not validation
    set).}
  \label{fig:overfitting-paper}
\end{figure}

Figure~\ref{fig:overfitting-paper}



{Three different data sets/patterns}
\begin{itemize}
\item We illustrate this using a single input/feature
  $x\in\mathbb R$.
\item We use a regression problem with outputs $y\in\mathbb R$.
\item Goal is to learn a function $f(x)\in\mathbb R$.
\end{itemize}



{Illustration of 4-fold cross-validation}
Randomly assign each observation a fold ID from 1 to 4.

{Illustration of subtrain/validation split}
\begin{itemize}
\item For validation fold 1, all observations with that fold ID are
  considered the validation set.
\item All other observations are considered the subtrain set.
\end{itemize}

{How to do this in practice? Use R with an IDE}
\begin{itemize}
\item Machine learning model training/evaluation is easy to code in
  programming languages which support vectors/matrices.
\item I recommend using R because it has so many useful packages
  (each one for a different machine learning model/algorithm).
\item Download R at \url{https://cloud.r-project.org/}
\item If you already use Emacs (classic text editor), I recommend
  using R inside Emacs Speaks Statistics, which is a great
  Interactive Development Environment (IDE) for R
  \url{http://ess.r-project.org/} Also see my screencasts about ESS,
  \url{https://www.youtube.com/playlist?list=PLwc48KSH3D1Onsed66FPLywMSIQmAhUYJ}
\item Otherwise, I recommend using RStudio, which is another popular
  IDE for R
  \url{https://rstudio.com/products/rstudio/download}
\end{itemize}

{CSV data tables for machine learning}
\begin{itemize}
\item One row for each observation.
\item One column for the output/label/y (in regression the label is a real
  number, in classification the label is a class/category).
\item The other columns should be inputs/features/X that will be used
  to predict the corresponding output/label/y.
\end{itemize}
  Example:
  \url{https://raw.githubusercontent.com/tdhock/2020-yiqi-summer-school/master/data_linear.csv}
\begin{verbatim}
               x            y
  1: -1.40694802  0.933196336
  2: -0.76725660  3.832773444
  3:  0.43712018  4.202983135
  4:  2.44924674 13.089055084
...
\end{verbatim}


{Download CSV data and get file names into R}
Download a copy of the github repo to get these data sets
\url{https://github.com/tdhock/2020-yiqi-summer-school/archive/master.zip}
Set working directory to the downloaded repo:
\begin{verbatim}
> setwd("~/teaching/2020-yiqi-summer-school")
> getwd()
[1] "/home/tdhock/teaching/2020-yiqi-summer-school"
\end{verbatim}
  Get a character vector of CSV data file names:
\begin{verbatim}
> csv.vec <- Sys.glob("data_*.csv")
> csv.vec
[1] "data_linear.csv"    "data_quadratic.csv"
[3] "data_sin.csv"      
\end{verbatim}

{Read each CSV data file into R}
\begin{verbatim}
sim.data.list <- list()
for(csv in csv.vec){
  pattern <- gsub("data_|.csv", "", csv)
  sim.dt <-  data.table::fread(csv)
  set.seed(1) # for reproducibility.
  fold.vec <- sample(rep(1:4, l=nrow(sim.dt)))
  sim.data.list[[pattern]] <- data.table::data.table(
    pattern, sim.dt, fold=factor(fold.vec))
} # combine into a single data table:
sim.data <- do.call(rbind, sim.data.list)
\end{verbatim}
\begin{itemize}
\item \texttt{gsub} removes prefix/suffix from CSV file name.
\item \texttt{data.table::fread} gets CSV data from file into R.
\item \texttt{set.seed} used to get consistent/reproducible results from the
  pseudo-random number generator (\texttt{sample}).
\item \texttt{data.table::fun} means use \texttt{fun} from
  \texttt{data.table} package, install it via
  \texttt{install.packages("data.table")}
\end{itemize}

{Result of reading CSV data into R}
  Result is:
\begin{verbatim}
> sim.data
     pattern          x          y fold
  1:  linear -1.4069480  0.9331963    4
  2:  linear -0.7672566  3.8327734    3
  3:  linear  0.4371202  4.2029831    1
  4:  linear  2.4492467 13.0890551    2
  5:  linear -1.7899084  2.0791987    3
 ---                                   
296:     sin  1.7838530  4.0502991    1
297:     sin -0.2683533 -0.1097264    1
298:     sin -0.5394955 -0.5539398    1
299:     sin  1.8652215 -0.2262517    4
300:     sin  0.6295997  8.8124249    4
\end{verbatim}

{Assign each observation to subtrain/validation set}
\begin{verbatim}
validation.fold <- 1
sim.data[, set := ifelse(
  fold==validation.fold, "validation", "subtrain")]
> sim.data
     pattern          x          y fold        set
  1:  linear -1.4069480  0.9331963    4   subtrain
  2:  linear -0.7672566  3.8327734    3   subtrain
  3:  linear  0.4371202  4.2029831    1 validation
  4:  linear  2.4492467 13.0890551    2   subtrain
  5:  linear -1.7899084  2.0791987    3   subtrain
 ---                                              
296:     sin  1.7838530  4.0502991    1 validation
297:     sin -0.2683533 -0.1097264    1 validation
298:     sin -0.5394955 -0.5539398    1 validation
299:     sin  1.8652215 -0.2262517    4   subtrain
300:     sin  0.6295997  8.8124249    4   subtrain
\end{verbatim}

{Neural network with one hidden layer, 20 hidden units}
  Use for loops to fit different neural network models for each data
  set and number of iterations. 
\begin{verbatim}
maxit.values <- 10^seq(0, 4)
pattern.values <- c("linear", "quadratic", "sin")
for(i in maxit.values)for(p in pattern.values){
  pattern.data <- sim.data[pattern==p]
  fit <- nnet::nnet(
    y ~ x,
    pattern.data[set=="subtrain"],
    size=20,     #hidden units
    linout=TRUE, #for regression
    maxit=i)     #max number of iterations
...
\end{verbatim}

  
{Overfitting}
\begin{itemize}
\item Happens when subtrain error/loss decreases but validation error
  increases (as a function of some hyper-parameter)
\item Here the hyper-parameter is the number of iterations of
  gradient descent, and overfitting starts after 10--100 iterations.
\item To maximize prediction accuracy you need to choose a
  hyper-parameter with minimal validation error/loss.
\item This optimal hyper-parameter will depend on the data set.
\item In this case that means choosing 10 iterations for the linear
  data set, and 100 iterations for the quadratic/sin data sets.
\end{itemize}

{Computing subtrain/validation loss curves}
\begin{verbatim}
...
  pattern.data[, pred.y := as.numeric(predict(
    fit, pattern.data))]
  loss.dt.list[[paste(pattern, maxit)]] <- data.table(
    pattern=p, maxit=i, pattern.data[, .(
      mse=mean((pred.y-y)^2)), by=set])
}
loss.dt <- do.call(rbind, loss.dt.list)
\end{verbatim}
Result is:  
\begin{verbatim}
      pattern        set maxit         mse
 1:    linear   subtrain     1  17.7785881
 2:    linear validation     1  23.0575528
...
29:       sin   subtrain 10000   0.6970455
30:       sin validation 10000  19.5051785
\end{verbatim}


{How does the neural network predict/learn?}
  \begin{eqnarray*}
    \text{hidden units: }\ h_j &=& \sigma(w_j x + b_j) \\
    \text{predicted output: }\  \texttt{o} = \hat y &=& \sum_{j=1}^u v_j h_j + \beta
  \end{eqnarray*}
  \begin{itemize}
  \item $x\in\mathbb R$ is the input/feature (\texttt{i1} in R notation below).
  \item $\sigma:\mathbb R\rightarrow [0,1]$ is the sigmoid activation
    function, $\sigma(z)=(1+e^{-z})^{-1}$.
  \item $h_j\in[0,1]$ are $u$ hidden units, e.g. $u\in\{20, 2\}$ (\texttt{h1},\texttt{h2}).
  \item $w_j,b_j,v_j,\beta\in\mathbb R$ are the weights which are
    learned using gradient descent to minimize the squared error, e.g. for $u=2$:
  \end{itemize}
  Result is:
\begin{verbatim}
> coef(fit)
     b->h1     i1->h1      b->h2     i1->h2 
-40.770433  -2.636863 -20.719333   2.629485 
      b->o      h1->o      h2->o 
  4.643852  18.973225  28.348342 
\end{verbatim}


{Comments on other model sizes}
  \begin{itemize}
  \item Model with 2 hidden units is less flexible: does not fit the subtrain
    data as well, and does not overfit as much.
  \item Model with 200 hidden units is more flexible: fits subtrain
    data even better, and overfits even more.
  \item Therefore model size / number of hidden units is also a
    hyper-parameter that can be used to control overfitting.
  \item R code is the same, just use \texttt{size=2} or 200 (or even
    better, I wrote a for loop over size values).
  \item Conclusion: you need to use a held-out validation set to
    choose the best values of hyper-parameters (e.g. number of
    iterations, hidden units).
  \end{itemize}

% QUIZ 1. purpose of train/subtrain/validation/test
% sets. 2. overfitting/underfitting. 3. data input format for
% ML. 4. cross-validation fold ID / test/train sets.

{Image classification}
  \begin{itemize}
  \item One of the most popular/successful applications of machine
    learning.
  \item Input: image file $x\in\mathbb R^{h\times w\times c}$ where
    $h$ is the height in pixels, $w$ is the width, $c$ is the number
    of channels, e.g. RGB image $c=3$ channels.
  \item In this tutorial we use images with $h=w=16$ pixels and $c=1$
    channel (grayscale, smaller values are darker).
  \item Output: class/category $y$ (from a finite set).
  \item In this tutorial there are ten image classes $y\in\{0, 1, \dots, 9\}$, one for each
    digit.
  \item Want to learn $f$ such that
  \item Code for figures in this section:
    \url{https://github.com/tdhock/2020-yiqi-summer-school/blob/master/figure-validation-loss.R}
  \end{itemize}

{Representation of digits in CSV}
  \begin{itemize}
  \item Each image/observation is one row.
  \item First column is output/label/class to predict.
  \item Other 256 columns are inputs/features (pixel intensity
    values).
  \end{itemize}
\begin{verbatim}
 1:  6 -1 -1  ... -1.000 -1.000   -1
 2:  5 -1 -1  ... -0.671 -0.828   -1
 3:  4 -1 -1  ... -1.000 -1.000   -1
 4:  7 -1 -1  ... -1.000 -1.000   -1
 5:  3 -1 -1  ... -0.883 -1.000   -1
 6:  6 -1 -1  ... -1.000 -1.000   -1
 7:  3 -1 -1  ... -1.000 -1.000   -1
 8:  1 -1 -1  ... -1.000 -1.000   -1
 9:  0 -1 -1  ... -1.000 -1.000   -1
10:  1 -1 -1  ... -1.000 -1.000   -1
11:  7 -1 -1  ... -1.000 -1.000   -1
12:  0 -1 -1  ... -1.000 -1.000   -1
\end{verbatim}

{Converting label column to matrix for neural network}
  This is a ``one hot'' encoding of the class labels.
\begin{verbatim}
zip.dt <- data.table::fread("zip.gz")
zip.y.mat <- keras::to_categorical(zip.dt$V1)

      0 1 2 3 4 5 6 7 8 9
 [1,] 0 0 0 0 0 0 1 0 0 0
 [2,] 0 0 0 0 0 1 0 0 0 0
 [3,] 0 0 0 0 1 0 0 0 0 0
 [4,] 0 0 0 0 0 0 0 1 0 0
 [5,] 0 0 0 1 0 0 0 0 0 0
 [6,] 0 0 0 0 0 0 1 0 0 0
 [7,] 0 0 0 1 0 0 0 0 0 0
 [8,] 0 1 0 0 0 0 0 0 0 0
 [9,] 1 0 0 0 0 0 0 0 0 0
[10,] 0 1 0 0 0 0 0 0 0 0
[11,] 0 0 0 0 0 0 0 1 0 0
[12,] 1 0 0 0 0 0 0 0 0 0
\end{verbatim}

{Conversion to array for input to neural network}
Use array function with all columns except first as data.  
\begin{verbatim}
zip.size <- 16
zip.X.array <- array(
  data = unlist(zip.dt[1:nrow(zip.dt),-1]),
  dim = c(nrow(zip.dt), zip.size, zip.size, 1))
\end{verbatim}
Need to specify dimensions of array:
\begin{itemize}
\item Observations: same as the number of rows in the CSV table.
\item Pixels wide: 16.
\item Pixels high: 16.
\item Channels: 1 (greyscale image).
\end{itemize}

{Linear model R code}
\begin{verbatim}
library(keras)
linear.model <- keras::keras_model_sequential() %>%
  keras::layer_flatten(
    input_shape = c(16, 16, 1)) %>%
  keras::layer_dense(
    units = 10,
    activation = 'softmax')
\end{verbatim}
  \begin{itemize}
  \item First layer must specify shape of inputs (here 16x16x1).
  \item \texttt{layer\_flatten} converts any shape to a single dimension
    of units (here 256).
  \item \texttt{layer\_dense} uses all units in the previous layer to
    predict each unit in the layer.
  \item \texttt{units=10} because there are ten possible classes for an output.
  \item \texttt{activation='softmax'} is required for the last/output layer in
    multi-class classification problems.
\end{itemize}

{Keras model compilation}
\begin{verbatim}
linear.model %>% keras::compile(
  loss = keras::loss_categorical_crossentropy,
  optimizer = keras::optimizer_adadelta(),
  metrics = c('accuracy')
)
\end{verbatim}
In \texttt{compile} you can specify
\begin{itemize}
\item a \texttt{loss} function, which is directly optimized/minimized
  in each iteration of the gradient descent learning algorithm. 
  \url{https://keras.io/api/losses/} 
\item an \texttt{optimizer}, which is the version of gradient descent
  learning algorithm to use. 
  \url{https://keras.io/api/optimizers/} 
\item an evaluation \texttt{metric} to monitor, not directly optimized
  via gradient descent, but usually more relevant/interpretable for
  the application (e.g. accuracy is the proportion of correctly
  predicted labels). \url{https://keras.io/api/metrics/} 
\end{itemize}
 
{Keras model fitting}
\begin{verbatim}
linear.model %>% keras::fit(
  zip.X.array, zip.y.mat,
  epochs = 50,
  validation_split = 0.2
)
\end{verbatim}
In \texttt{fit} you can specify
\begin{itemize}
\item Train data inputs \texttt{zip.X.array} and outputs
  \texttt{zip.y.mat} (required).
\item Number of full passes of gradient descent through the subtrain
  data (\texttt{epochs}). In each epoch the gradient with respect to
  each subtrain observation is computed once.
\item \texttt{validation\_split=0.2} which means to use 80\% subtrain
  (used for gradient descent parameter updates), 20\% validation (used
  for hyper-parameter selection). 
\end{itemize}

{Sparse (convolutional) model R code}
\begin{verbatim}
library(keras)
conv.model <- keras_model_sequential() %>%
  layer_conv_2d(
    input_shape = dim(zip.X.array)[-1],
    filters = 20,
    kernel_size = c(3,3),
    activation = 'relu') %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dense(units = 100, activation = 'relu') %>% 
  layer_dense(
    units = ncol(zip.y.mat), 
    activation = 'softmax')
\end{verbatim}
  \begin{itemize}
  \item Sparse: few inputs are used to predict each unit in
    \texttt{layer\_conv\_2d}.
  \item Exploits structure of image data to make learning
    easier/faster.
  \end{itemize}

{Dense (fully connected) neural network R code}
\begin{verbatim}
library(keras)
dense.model <- keras_model_sequential() %>%
  layer_flatten(
    input_shape = dim(zip.X.array)[-1]) %>%
  layer_dense(units = 100, activation = 'relu') %>% 
  layer_dense(units = 100, activation = 'relu') %>% 
  layer_dense(units = 100, activation = 'relu') %>% 
  layer_dense(units = 100, activation = 'relu') %>%
  layer_dense(units = 100, activation = 'relu') %>%
  layer_dense(units = 100, activation = 'relu') %>% 
  layer_dense(units = 100, activation = 'relu') %>%   
  layer_dense(units = 100, activation = 'relu') %>% 
  layer_dense(
    units = ncol(zip.y.mat), 
    activation = 'softmax')
\end{verbatim}

{4-fold cross-validation for model evaluation}
  Does the convolutional model provide more accurate predictions on
  unseen test data? First randomly assign a fold ID to each image/observation,
  then for each test fold ID from 1 to 4:
  \begin{itemize}
  \item Hold out the images/observations with the test fold ID as a
    test set (not used at all for model training).
  \item Use the other images/observations to train all three models
    using \texttt{validation\_split=0.2}.
  \item Plot the validation loss curve as a function of the number of
    epochs, and select the number of epochs which minimizes the
    validation loss (hyper-parameter learning).
  \item Re-train with \texttt{epochs=}the learned number of epochs and
    \texttt{validation\_split=0} (all train data used for gradient
    descent parameter updates).
  \item Finally compute the prediction accuracy with respect to the
    held-out test set.
  \end{itemize}
Plot accuracy values (or mean/sd) after learning/testing four times
(one for each test fold).

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{figure-test-accuracy-both}
  \caption{test accuracy values}
  \label{fig:test-accuracy}
\end{figure}
 
{Accuracy rates for each test fold}
  \begin{itemize}
  \item Always a good idea to compare with the trivial baseline model which always
    predicts the most frequent class in the train set. (ignoring all
    inputs/features) 
  \item Here we see that the baseline is much less accurate than the
    three learned models, so they are clearly learning something non-trivial.
  \item Code for test accuracy figures:
    \url{https://github.com/tdhock/2020-yiqi-summer-school/blob/master/figure-test-accuracy.R}
  \end{itemize}

{Zoom to learned models}
\begin{itemize}
\item Dense neural network slightly more accurate
  than linear model, convoluational significantly more
  accurate than others.
\item Conclusion: convolutional neural network should be preferred
  for most accurate predictions in these data.
\item Maybe not the same conclusion in other data sets, with the
  same models. (always need to do cross-validation experiments to
  see which model is best in any given data set)
\item Maybe other models/algorithms would be even more accurate in
  these data. (more/less layers, more/less units, completely
  different algorithm such as random forests, boosting, etc)
\end{itemize}

\bibliographystyle{abbrvnat}
\bibliography{refs} 

\end{document}
